{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>**BIG** **DATA** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1)\n",
    "# instanciation\n",
    "\n",
    "Spark = SparkSession.builder.master(\"local\").appName(\"FolksDF\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/Users/carrenolan/Desktop/Etudes/Python/Moez/TP_WALMART/Script/data/walmart_stock.csv;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-92faf73cf5c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Importation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mWalmart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delimiter\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/walmart_stock.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mWalmart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/Users/carrenolan/Desktop/Etudes/Python/Moez/TP_WALMART/Script/data/walmart_stock.csv;"
     ]
    }
   ],
   "source": [
    "#2)\n",
    "#Importation \n",
    "Walmart = Spark.read.option(\"header\",'true').option(\"delimiter\",\",\").csv(\"data/walmart_stock.csv\")\n",
    "Walmart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3)\n",
    "Walmart.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4) le schema \n",
    "\n",
    "Walmart.printSchema()\n",
    "#nullable =true : autorise les valeurs nulles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+---------+------------------+--------+------------------+--------------------+\n",
      "|      Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|            HV_Ratio|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+--------------------+\n",
      "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|4.819714653321546E-6|\n",
      "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|6.290848613094555E-6|\n",
      "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|4.669412994783916E-6|\n",
      "|2012-01-06|         59.419998|59.450001|58.869999|              59.0| 8069400|          51.45922|7.367338463826307E-6|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5)\n",
    "Walmart2 = Walmart.withColumn('HV_Ratio',F.col('High')/F.col('Volume')) #Preciser que ce sont des colonnes sinon spark comprend pas\n",
    "Walmart2.head()#La premiere ligne\n",
    "Walmart2.show(4)#les 4 premieres lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6)\n",
    "Walmart2.createOrReplaceTempView(\"WalmartSQL\") #Transformation du data frame en table !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date='2015-01-13')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SOLUTION 1\n",
    "#SQL\n",
    "Spark.sql(\"SELECT Date FROM WalmartSQL ORDER BY High desc\").first()#Montre la premiere data apres avoir trié \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Date|\n",
      "+----------+\n",
      "|2015-01-13|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Solution 2\n",
    "Spark.sql(\"SELECT Date from WalmartSQL ORDER BY High Desc limit 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date='2015-01-13')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DSL\n",
    "Walmart2.orderBy(F.col(\"High\").desc()).select(F.col(\"Date\")).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date='2015-01-13')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Walmart2.select(F.col(\"Date\")).orderBy(F.col(\"High\").desc()).head()#Il n'y a pas d'ordre en fait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date='2015-01-13')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On prefere ecrire comme ceci\n",
    "Walmart2.select(F.col(\"Date\")) \\\n",
    "        .orderBy(F.col(\"High\") \\\n",
    "        .desc()) \\\n",
    "        .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    Moyenne_Close|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#7 la moyenne de close en sql et dsl\n",
    "#SQL\n",
    "Spark.sql(\"Select mean(Close) as Moyenne_Close from WalmartSQL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|            Close|\n",
      "+-------+-----------------+\n",
      "|   mean|72.38844998012726|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DSL\n",
    "#Solution 1\n",
    "Walmart2.select(F.col(\"Close\"))\\\n",
    "        .summary(\"mean\") \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    Moyenne_Close|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Solution 2\n",
    "Walmart2.agg(F.mean(\"Close\")\\\n",
    "        .alias(\"Moyenne_Close\"))\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|min(Volume)|max(Volume)|\n",
      "+-----------+-----------+\n",
      "|   10010500|    9994400|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#7 \n",
    "#SQL\n",
    "Spark.sql(\"Select min(Volume),max(Volume) from WalmartSQL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|summary|  Volume|\n",
      "+-------+--------+\n",
      "|    min|10010500|\n",
      "|    max| 9994400|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DSL\n",
    "#Solution 1\n",
    "Walmart2.select(F.col(\"Volume\"))\\\n",
    "        .summary(\"min\",\"max\") \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|max(Volume)|min(Volume)|\n",
      "+-----------+-----------+\n",
      "|    9994400|   10010500|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Solution 1\n",
    "Walmart2.select(F.max(\"Volume\"),F.min(\"Volume\"))\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|count(Date)|\n",
      "+-----------+\n",
      "|         81|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#8 \n",
    "#SQL\n",
    "Spark.sql(\"Select count(Date) from WalmartSQL  where Close < '60' \").show() #'60' pas obligé sur cette version mias preferable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|count(Date)|\n",
      "+-----------+\n",
      "|         81|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DSL\n",
    "#Solution 1\n",
    "Walmart2.filter(F.col(\"Close\") < '60') \\\n",
    "        .agg(F.count(\"Date\"))\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Solution 2\n",
    "Walmart2.filter(F.col(\"Close\") < '60') \\\n",
    "        .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Percentage|\n",
      "+----------+\n",
      "|      9.14|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#9 What percentage of the time was the High greater than 80 dollars ?(In other words, (Number of Days High>80)/(Total Days in the dataset))\n",
    "#SQL\n",
    "\n",
    "Spark.sql(\"select round((select count(*) from WalmartSQL where High > '80')/(count(*))* 100, 2) as Percentage from WalmartSQL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|Comptage|\n",
      "+--------+\n",
      "|    9.14|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DSL \n",
    "Temp = Walmart2.filter(F.col(\"High\")> '80')\\\n",
    "        .agg(F.count(\"*\").alias(\"Comptage\")) \\\n",
    "        .collect()[0][0]\n",
    "\n",
    "Walmart2.agg(F.round((Temp/F.count(\"*\")*100),2).alias(\"Comptage\"))\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|max(High)|Annee|\n",
      "+---------+-----+\n",
      "|75.190002| 2016|\n",
      "|77.599998| 2012|\n",
      "|88.089996| 2014|\n",
      "|81.370003| 2013|\n",
      "|90.970001| 2015|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#11-What is the max High per year?\n",
    "#SQL\n",
    "\n",
    "Spark.sql(\"select max(High),SUBSTR(Date, 1, 4) as Annee from WalmartSQL group by Annee\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      Date|Annee|\n",
      "+----------+-----+\n",
      "|2012-01-03| 2012|\n",
      "|2012-01-04| 2012|\n",
      "|2012-01-05| 2012|\n",
      "|2012-01-06| 2012|\n",
      "|2012-01-09| 2012|\n",
      "+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+---------+\n",
      "|Annee|max(High)|\n",
      "+-----+---------+\n",
      "| 2016|75.190002|\n",
      "| 2012|77.599998|\n",
      "| 2014|88.089996|\n",
      "| 2013|81.370003|\n",
      "| 2015|90.970001|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DSL \n",
    "\n",
    "#Partie 1 : Crée une nouvelle colonne Annee qui extrait l'année depuis la date\n",
    "Walmart3 = Walmart2.withColumn(\"Annee\",F.substring(\"Date\",1,4))\n",
    "Walmart3.select(\"Date\",\"Annee\").show(5)#On fait pas le show direct lorsque qu'on crée un dataframe mais apres\n",
    "#Partie 2 :  Retrouver le maximum du prix pour chaque année\n",
    "Walmart3.groupBy(F.col(\"Annee\")) \\\n",
    "        .agg(F.max(\"High\")) \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------------+\n",
      "|Annee|Mois|         Moy_Close|\n",
      "+-----+----+------------------+\n",
      "| 2012|  01|        60.2354999|\n",
      "| 2012|  02|            60.898|\n",
      "| 2012|  03|60.433636818181796|\n",
      "| 2012|  04|60.149000150000006|\n",
      "| 2012|  05|61.456363409090905|\n",
      "| 2012|  06| 67.50380961904762|\n",
      "| 2012|  07| 72.40666661904763|\n",
      "| 2012|  08| 73.04478265217392|\n",
      "| 2012|  09| 74.18157921052631|\n",
      "| 2012|  10| 75.30619061904761|\n",
      "| 2012|  11| 71.10952333333333|\n",
      "| 2012|  12| 69.71100009999999|\n",
      "| 2013|  01| 69.09476142857143|\n",
      "| 2013|  02| 70.62315857894738|\n",
      "| 2013|  03| 73.43649940000002|\n",
      "| 2013|  04| 77.68954572727273|\n",
      "| 2013|  05| 77.81636368181817|\n",
      "| 2013|  06| 74.97800020000001|\n",
      "| 2013|  07| 77.11545418181818|\n",
      "| 2013|  08| 75.22409204545455|\n",
      "+-----+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#What is the average Close for each Calendar Month? In other words, across all the years, what is the average Close price for Jan,Feb, Mar, etc... Your result will have a value for each of these months.\n",
    "Spark.sql(\"select SUBSTR(Date, 1, 4) as Annee, SUBSTR(Date, 6, 2) as Mois, mean(Close) as Moy_Close from WalmartSQL group by Annee , Mois order by Annee, Mois\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
